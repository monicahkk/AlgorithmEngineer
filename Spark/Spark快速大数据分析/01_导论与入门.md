## 导论 ##

### 软件栈 ###

![Spark 软件栈](D:\MyDocuments\Typora\05-spark\Spark快速大数据分析\Spark软件栈.png)

**Spark Core**：实现Spark 的基本功能，包含任务调度、内存管理、错误恢复、与存储系统交互等模块，包含了对__弹性分布式数据集__（resilient distributed dataset，简称 RDD）的 API 定义

**Spark SQL**： 用来操作结构化数据的程序包

**Spark Streaming**：对实时数据进行流式计算的组件，Spark Streaming 支持与Spark Core 同级别的容错性、吞吐量以及可伸缩性

**MLlib**：机器学习（ML）功能的程序库，提供了很多种机器学习算法，提供了模型评估、数据导入等额外的支持功能，还提供了一些更底层的机器学习原语，包括一个通用的梯度下降优化算法

**GraphX**：操作图（比如社交网络的朋友关系图）的程序库，可以进行并行的图计算，支持针对图的各种操作，以及一些常用图算法

**集群管理器（cluster manager）**：可以高效地在一个计算节点到数千个计算节点之间伸缩计算，支持在Hadoop YARN、Apache Mesos，以及 Spark 自带的一个简易调度器，叫作独立调度器

**用途**： __数据科学__应用和__数据处理__应用

******

**下载**：安装到不带空格的路径下

```spark
cd ~
tar -xf spark-1.2.0-bin-hadoop2.4.tgz
cd spark-1.2.0-bin-hadoop2.4
ls
```

<!--x 标记指定 tar 命令执行解压缩操作，f 标记则指定压缩包的文件名-->

Spark 带有交互式的 shell，可以作即时数据分析

```spark
bin/pyspark  # 打开 Python 版本的 Spark shell
bin/spark-shell  # 打开 Scala 版本的 shell

# 调整日志的级别来控制输出的信息量
# 创建一个名为 log4j.properties 的文件来管理日志设置
# 日志设置文件的模版: log4j.properties.template
# log4j.rootCategory=INFO, console 改为 log4j.rootCategory=WARN, console

# 环境变量 IPYTHON 的值设为 1，就可以使用 IPython
IPYTHON=1 ./bin/pyspark

# 使用 IPython Notebook，也就是 Web 版的 IPython
IPYTHON_OPTS="notebook" ./bin/pyspark

# Windows 上
set IPYTHON=1
bin\pyspark

# Python 行数统计
lines = sc.textFile("README.md") # 创建一个名为lines的RDD
lines.count() # 统计RDD中的元素个数
lines.first() # 这个RDD中的第一个元素，也就是README.md的第一行

# Scala 行数统计
val lines = sc.textFile("README.md") // 创建一个名为lines的RDD
lines.count() // 统计RDD中的元素个数
lines.first() // 这个RDD中的第一个元素，也就是README.md的第一行

# Ctrl-D: 退出任一 shell
```

**核心概念**：

- 每个 Spark 应用都由一个驱动器程序（driver program）来发起集群上的各种并行操作

- 驱动器程序包含应用的 main 函数，并且定义了集群上的分布式数据集，还对这些分布式数据集应用了相关操作

- 驱动器程序通过一个 SparkContext 对象来访问 Spark。这个对象代表对计算集群的一个连接

- shell 启动时已经自动创建了一个 SparkContext 对象，是一个叫作 sc 的变量

  ```spark
  # 查看变量sc
  sc
  ```

- 一旦有了 SparkContext，就可以用它来创建 RDD

- 要执行操作，驱动器程序一般要管理多个执行器（executor）节点 -- 如果我们在集群上运行 count() 操作，那么不同的节点会统计文件的不同部分的行数

- 有很多用来传递函数的 API，可以将对应操作运行在集群上

  ```spark
  # Python 版本筛选的例子
  lines = sc.textFile("README.md")
  pythonLines = lines.filter(lambda line: "Python" in line)
  pythonLines.first()
  
  # Scala 版本筛选的例子
  val lines = sc.textFile("README.md") // 创建一个叫lines的RDD 
  val pythonLines = lines.filter(line => line.contains("Python"))
  pythonLines.first()
  ```


**独立应用**: 需要自行初始化SparkContext

- Java和Scala中，添加一个对于spark-core工件的Maven依赖 

- 在 Python 中，可以把应用写成 Python 脚本，需要使用 Spark 自带的 bin/sparksubmit 脚本来运行。spark-submit 脚本会帮我们引入 Python 程序的 Spark 依赖。

  `bin/spark-submit my_script.py`

  <!--Maven 是一个流行的包管理工具，可以用于任何基于 Java 的语言，让你可以连接公共仓库中的程序库-->

**初始化SparkContext**：

- 先创建一个 SparkConf 对象来配置应用

- 后基于这个SparkConf 创建一个 SparkContext 对象

  ```
  # 在 Python 中初始化 Spark
  from pyspark import SparkConf, SparkContext
  conf = SparkConf().setMaster("local").setAppName("My App")
  sc = SparkContext(conf = conf)
  
  # 在 Scala 中初始化 Spark
  import org.apache.spark.SparkConf
  import org.apache.spark.SparkContext
  import org.apache.spark.SparkContext._
  val conf = new SparkConf().setMaster("local").setAppName("My App")
  val sc = new SparkContext(conf)
  
  # 在 Java 中初始化 Spark
  import org.apache.spark.SparkConf;
  import org.apache.spark.api.java.JavaSparkContext;
  SparkConf conf = new SparkConf().setMaster("local").setAppName("My App");
  JavaSparkContext sc = new JavaSparkContext(conf);
  ```

  传递两个参数:

  - 集群 URL：告诉 Spark 如何连接到集群上
    - local: 让 Spark 运行在单机单线程上而无需连接到集群
  - 应用名：当连接到一个集群时，可以帮助你在
    集群管理器的用户界面中找到你的应用

  **创建 RDD 并操控**

  **关闭 Spark**: 

  - 调用 SparkContext 的 stop() 方法
  - 直接退出应用（比如通过System.exit(0) 或者 sys.exit()）

**构建应用**  P40

