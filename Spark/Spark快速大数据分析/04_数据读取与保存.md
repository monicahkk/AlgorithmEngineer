## 数据读取与保存
Spark 可以通过 Hadoop MapReduce 所使用的 InputFormat 和OutputFormat 接口访问数据

#### 三类常见的数据源：

- 文件格式与文件系统： 存储在本地文件系统或分布式文件系统的数据
- Spark SQL中的结构化数据源
- 数据库与键值存储

##### 文件格式
| 格式名称 | 结构化 | 备注 |
| --- | --- | --- |
| 文本文件 | 否 | 普通的文本文件，每行一条记录 |
| JSON | 半结构化 | 常见的基于文本的格式，半结构化；大多数库都要求每行一条记录 |
| CSV | 是 | 非常常见的基于文本的格式，通常在电子表格应用中使用 |
| SequenceFiles | 是 | 一种用于键值对数据的常见 Hadoop 文件格式 |
| Protocol buffers | 是 | 一种快速、节约空间的跨语言格式 |
| 对象文件 | 是 | 用来将 Spark 作业中的数据存储下来以让共享的代码读取。改变类的时候它会失效，因为它依赖于 Java 序列化 |

- 除了 Spark 中直接支持的输出机制，还可以对键数据（或成对数据）使用 Hadoop 的新旧文件 API
  - 由于 Hadoop 接口要求使用键值对数据，所以也只能这样用，即使有些格式事实上忽略了键
  - 对于那些会忽视键的格式，通常使用假的键（比如 null）

1. 文本文件

   当我们将一个文本文件读取为 RDD 时，输入的每一行都会成为 RDD 的一个元素；也可以将多个完整的文本文件一次性读取为一个 pair RDD，其中键是文件名，值是文件内容

   1.1 读取文本文件

```SPARQL
# 使用文件路径作为参数调用 SparkContext 中的 textFile() 函数
# 在 Python 中读取一个文本文件
input = sc.textFile("file:///home/holden/repos/spark/README.md")

# 多个输入文件以一个包含数据所有部分的目录的形式出现，2方法
# 1. 使用 textFile 函数，传递目录作为参数，这样会把各部分都读取到RDD中
# 2. 文件足够小，可以使用 SparkContext.wholeTextFiles()：方法会返回一个 pair RDD，其中键是输入文件的文件名；在每个文件表示一个特定时间段内的数据时非常有用
```

   1.2 保存文本文件

```SPARQL
# saveAsTextFile() 方法接收一个路径，并将RDD 中的内容都输入到路径对应的文件中
# Spark 将传入的路径作为目录对待，会在那个目录下输出多个文件
# 在这个方法中，我们不能控制数据的哪一部分输出到哪个文件中，不过有些输出格式支持控制
result.saveAsTextFile(outputFil)
```

2. JSON

   读取 JSON 数据的最简单的方式是将数据作为文本文件读取，然后使用 JSON 解析器来对 RDD 中的值进行映射操作

   2.1 读取json

```python 3
# 如果你有跨行的JSON 数据，你就只能读入整个文件，然后对每个文件进行解析
# 如果在你使用的语言中构建一个 JSON 解析器的开销较大，你可以使用 mapPartitions() 来重用解析器
# 内建的库 https://docs.python.org/2/library/json.html
import json
data = input.map(lambda x: json.loads(x))

# 处理格式不正确的记录有可能会引起很严重的问题
# 对于小数据集来说，可以接受在遇到错误的输入时停止程序（程序失败）；对于大规模数据集来说，格式错误是家常便饭
# 如果选择跳过格式不正确的数据，你应该尝试使用累加器来跟踪错误的个数
```

   2.2 保存JSON

```python 3
# 使用之前将字符串 RDD 转为解析好的 JSON 数据的库，将由结构化数据组成的 RDD 转为字符串 RDD，然后使用 Spark 的文本文件 API 写出去
(data.filter(lambda x: x["lovesPandas"]).map(lambda x: json.dumps(x)).saveAsTextFile(outputFile))
```

3. 逗号分隔值与制表符分隔值

   - TSV 文件中用制表符隔开，记录通常是一行一条，不过也不总是这样，有时也可以跨行

   - CSV 文件和 TSV 文件有时支持的标准并不一致，主要是在处理换行符、转义字符、非 ASCII 字符、非整数值等方面

   - CSV 原生并不支持嵌套字段，所以需要手动组合和分解特定的字段

   - 与 JSON 中的字段不一样的是，这里的每条记录都没有相关联的字段名，只能得到对应的序号。常规做法是使用第一行中每列的值作为字段名

	3.1 读取CSV

```python 3
# 先把文件当作普通文本文件来读取数据，再对数据进行处理
# 如果恰好你的 CSV 的所有数据字段均没有包含换行符，你也可以使用 textFile() 读取并解析数据
# 在 Python 中使用 textFile() 读取 CSV
import csv
import String

def loadRecord(line):
     """解析一行CSV记录"""
     input = StringIO.StringIO(line)
     reader = csv.DictReader(input, fieldnames=["name", "favouriteAnimal"])
     return reader.next()
input = sc.textFile(inputFile).map(loadRecord)

# 如果在字段中嵌有换行符，就需要完整读入每个文件，然后解析各段
# 如果每个文件都很大，读取和解析的过程可能会很不幸地成为性能瓶颈
# 在 Python 中完整读取 CSV
def loadRecords(fileNameContents):
     """读取给定文件中的所有记录"""
     input = StringIO.StringIO(fileNameContents[1])
     reader = csv.DictReader(input, fieldnames=["name", "favoriteAnimal"])
     return reader
fullFileData = sc.wholeTextFiles(inputFile).flatMap(loadRecords)
```

​	3.2 保存CSV

```python 3
# 可以通过重用输出编码器来加速
# 在 CSV 中我们不会在每条记录中输出字段名，因此为了使输出保持一致，需要创建一种映射关系 —— 一种简单做法是写一个函数，用于将各字段转为指定顺序的数组
# 在Python 中，如果输出字典，CSV 输出器会根据创建输出器时给定的 fieldnames 的顺序帮我们完成这一行为
# 在 Python 中写 CSV
def writeRecords(records):
     """写出一些CSV记录"""
     output = StringIO.StringIO()
     writer = csv.DictWriter(output, fieldnames=["name", "favoriteAnimal"])
     for record in records:
     	writer.writerow(record)
     return [output.getvalue()]
pandaLovers.mapPartitions(writeRecords).saveAsTextFile(outputFile)
```

4. SequenceFile

   - SequenceFile 是由没有相对关系结构的键值对文件组成的常用 Hadoop 格式。SequenceFile文件有同步标记，Spark 可以用它来定位到文件中的某个点，然后再与记录的边界对齐

   - SequenceFile 也是Hadoop MapReduce 作业中常用的输入输出格式

   - Hadoop 的 RecordReader 会为每条记录重用同一个对象，因此直接调用 RDD的 cache 会导致失败；实际上，你只需要使用一个简单的 map() 操作然后将结果缓存即可

     | Scala类型 | Java类型 | Hadoop Writable类 |
     | --- | --- | --- |
     | Int | Integer | IntWritable 或 VIntWritable2 |
     | Long | Long | LongWritable 或 VLongWritable2 |
     | Float | Float | FloatWritable |
     | Double | Double | DoubleWritable |
     | Boolean | Boolean | BooleanWritable |
     | Array[Byte] | byte[] | BytesWritable |
     | String | String | Text |
     | Array[T] | T[] | ArrayWritable<TW>3 |
     | List[T] | List<T> | ArrayWritable<TW>3 |
     | Map[A, B] | Map<A, B> | MapWritable<AW, BW>3 |
     
     <!-- 整型和长整型通常存储为定长的形式, 如果有大量的小数据，应该使用可变长的类型 VIntWritable 和 VLongWritable，它们可以在存储较小数值时使用更少的位-->
     
     <!-- Spark 1.1加入了在 Python 中读取和保存 SequenceFile 的功能,还是需要使用 Java或 Scala 来实现自定义 Writable 类。Spark 的 Python API 只能将Hadoop 中存在的基本 Writable 类型转为 Python 类型，并尽量基于可用的 getter 方法处理别的类型 -->
     
	
	4.1 读取SequenceFile

```python 3
# Spark 有专门用来读取 SequenceFile 的接口。在 SparkContext 中，可以调用sequenceFile(path, keyClass, valueClass, minPartitions)。
# SequenceFile 使用 Writable 类，因此 keyClass 和 valueClass 参数都必须使用正确的 Writable 类
# 在 Python 读取 SequenceFile
val data = sc.sequenceFile(inFile,"org.apache.hadoop.io.Text", "org.apache.hadoop.io.IntWritable")
```

​		4.2 保存SequenceFile

```SPARQL
# 如果键和值不能自动转为 Writable 类型，或者想使用变长类型（比VIntWritable），就可以对数据进行映射操作，在保存之前进行类型转
```

5. 对象文件

   允许存储只包含值的 RDD。和SequenceFile 不一样的是，对象文件是使用 Java 序列化写出的

   如果你修改了你的类——比如增减了几个字段——已经生成的对象文件就不再可读了

   注意:
    1. 和普通的 SequenceFile 不同，对于同样的对象，对象文件的输出和 Hadoop 的输出不一样
    2. 与其他文件格式不同的是，对象文件通常用于 Spark 作业间的通信
    3. Java 序列化有可能相当慢

    对象文件在 Python 中无法使用，不过 Python 中的 RDD 和 SparkContext 支持 saveAsPickleFile()pickleFile() 方法作为替代。这使用了 Python 的 pickle 序列化库
```SPARQL
 saveAsObjectFile # 保存对象文件
 objectFile(file_url) # 读回对象文件,SparkContext.objectFile()
```

##### Hadoop输入输出格式
  - 读取其他Hadoop输入格式
    newAPIHadoopFile接收一个路径以及三个类: 
      - “格式”类，代表输入格式 <=> hadoopFile() 则用于使用旧的 API 实现的 Hadoop 输入格式
      - 键的类
      - 值的类
      - 设定额外的 Hadoop 配置属性，也可以传入一个 conf 对象
```SPARQL
# KeyValueTextInputFormat 是最简单的 Hadoop 输入格式之一，可以用于从文本文件中读取键值对数据,每一行都会被独立处理，键和值之间用制表符隔开

# 在 Scala 中使用老式 API 读取 KeyValueTextInputFormat()
val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](inputFile).map{
 case (x, y) => (x.toString, y.toString)
}

# 使用自定义 Hadoop 输入格式来读取 JSON 数据
# Twitter 的 Elephant Bird 包（https://github.com/twitter/elephant-bird）支持很多种数据格式，包括 JSON、Lucene、Protocol Buffer 相关的格式等。这个包也适用于新旧两种 Hadoop 文件 API
# 在 Scala 中使用 Elephant Bird 读取 LZO 算法压缩的 JSON 文件
val input = sc.newAPIHadoopFile(inputFile, classOf[LzoJsonInputFormat],classOf[LongWritable], classOf[MapWritable], conf) # "输入"中的每个MapWritable代表一个JSON对象
```
    - 使用旧的 Hadoop API 读取文件需要提供旧式 InputFormat 类
  - 保存Hadoop输出格式
```SPARQL
# 在 Java 保存 SequenceFile 
# 新接口（saveAsNewAPIHadoopFile）的调用方法也是类似的
public static class ConvertToWritableTypes implements 
  PairFunction<Tuple2<String, Integer>, Text, IntWritable> { 
  public Tuple2<Text, IntWritable> call(Tuple2<String, Integer> record) { 
    return new Tuple2(new Text(record._1), new IntWritable(record._2)); 
 } 
} 
JavaPairRDD<String, Integer> rdd = sc.parallelizePairs(input); 
JavaPairRDD<Text, IntWritable> result = rdd.mapToPair(new ConvertToWritableTypes()); 
result.saveAsHadoopFile(fileName, Text.class, IntWritable.class, 
  SequenceFileOutputFormat.class);
```
  - 非文件系统数据源
    - 使用 hadoopDataset/saveAsHadoopDataSet 和 newAPIHadoopDataset/saveAsNewAPIHadoopDataset
      - hadoopDataset() 这一组函数只接收一个 Configuration 对象，这个对象用来设置访问数据源所必需的 Hadoop 属性
  - protocol buffer (PB，https://github.com/google/protobuf)
    - 网站(https://developers.google.com/protocol-buffers ）
```SPARQL
# PB 定义示例
message Venue {
  required int32 id = 1;
  required string name = 2;
  required VenueType type = 3;
  optional string address = 4;

  enum VenueType {
    COFFEESHOP = 0;
    WORKPLACE = 1;
    CLUB = 2;
    OMNOMNOM = 3;
    OTHER = 4;
  }
}
message VenueResponse {
  repeated Venue results = 1;
}
```
  - 文件压缩
    - Spark 原生的输入方式（textFile 和 sequenceFile）可以自动处理一些类型的压缩
    - 尽管 Spark 的 textFile() 方法可以处理压缩过的输入，但即使输入数据被以可分割读取的方式压缩，Spark 也不会打开 splittable。因此，如果你要读取单个压缩过的输入，最好不要考虑使用 Spark 的封装，而是使用newAPIHadoopFile 或者 hadoopFile，并指定正确的压缩编解码器
|格式|可分割|平均压缩速度|文本文件压缩效率|Hadoop压缩编解码器|纯Java实现|原生|备注|
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|gzip|否|快|高|org.apache.hadoop.io.compress.GzipCodec|是|是||
|lzo|是|非常快|中等|com.hadoop.compression.lzo.LzoCodec|是|是|需要在每个节点上安装 LZO|
|bzip2|是|慢|非常高|org.apache.hadoop.io.compress.Bzip2Codec|是|是|为可分割版本使用纯 Java|
|zlib|否|慢|中等|org.apache.hadoop.io.compress.DefaultCodec|是|是|Hadoop 的默认压缩编解码器|
|Snappy|否|非常快|低|org.apache.hadoop.io.compress.SnappyCodec|否|是|Snappy 有纯 Java的移植版，但是在 Spark/Hadoop中不能用|

##### 文件系统
  - 本地/“常规”文件系统
    - Spark 支持从本地文件系统中读取文件，不过它要求文件在集群中所有节点的相同路径下都可以找到
```scala
# 在 Scala 中从本地文件系统读取一个压缩的文本文件
val rdd = sc.textFile("file:///home/holden/happypandas.gz")
```
    - 如果文件还没有放在集群中的所有节点上,推荐的方法是将文件先放到像 HDFS、NFS、S3 等共享文件系统上
  - Amazon S3
    - 首先把你的 S3 访问凭据设置为 AWS_ACCESS_KEY_ID 和AWS_SECRET_ACCESS_KEY 环境变量
    - 将一个以 s3n:// 开头的路径以 s3n://bucket/path-within-bucket 的形式传给Spark 的输入方法
  - HDFS
    - 将输入输出路径指定为 hdfs://master:port/path
    - HDFS 协议随 Hadoop 版本改变而变化，因此如果你使用的 Spark 是依赖于另一个版本的 Hadoop 编译的，那么读取会失败。默认情况下，Spark 基于Hadoop 1.0.4 编译 7。如果从源代码编译，你可以在环境变量中指定 SPARK_HADOOP_VERSION= 来基于另一个版本的 Hadoop 进行编译；也可以直接下载预编译好的 Spark 版本。你可以根据运行 hadoop version 的结果来获得环境变量要设置的值

##### Spark SQL中的结构化数据
  - 把一条 SQL 查询给 Spark SQL，让它对一个数据源执行查询（选出一些字段或者对字段使用一些函数），然后得到由 Row 对象组成的 RDD，每个 Row 对象表示一条记录
    - 在 Java 和 Scala 中，Row 对象的访问是基于下标的
    - 每个 Row 都有一个get() 方法，会返回一个一般类型让我们可以进行类型转换
    - 有针对常见基本类型的 专 用 get() 方 法（ 例 如 getFloat()、getInt()、getLong()、getString()、getShort()、getBoolean() 等）
    - 在 Python 中，可以使用 row[column_number] 以及 row.column_name 来访问元素
  - Apache Hive
    - 要把 Spark SQL 连接到已有的 Hive 上，你需要提供 Hive 的配置文件。你需要将 hive-site.xml 文件复制到 Spark 的 ./conf/ 目录下,再创建出 HiveContext 对象，也就是 Spark SQL 的入口
```python
# 用 Python 创建 HiveContext 并查询数据
from pyspark.sql import HiveContext
hiveCtx = HiveContext(sc)
rows = hiveCtx.sql("SELECT name, age FROM users")
firstRow = rows.first()
print firstRow.name
```
```java
// 例 5-31：用 Scala 创建 HiveContext 并查询数据
import org.apache.spark.sql.hive.HiveContext
val hiveCtx = new org.apache.spark.sql.hive.HiveContext(sc)
val rows = hiveCtx.sql("SELECT name, age FROM users")
val firstRow = rows.first()
println(firstRow.getString(0)) // 字段0是name字段
```
```java
// 例 5-32：用 Java 创建 HiveContext 并查询数据
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SchemaRDD;
HiveContext hiveCtx = new HiveContext(sc);
SchemaRDD rows = hiveCtx.sql("SELECT name, age FROM users");
Row firstRow = rows.first();
System.out.println(firstRow.getString(0)); // 字段0是name字段
```

##### JSON
  - 创建一个 HiveContext,然后使用 HiveContext.jsonFile 方法来从整个文件中获取由 Row 对象组成的 RDD
  - 除了使用整个 Row 对象，你也可以将 RDD注册为一张表，然后从中选出特定的字段
```json
// 例 5-33：JSON 中的示例推文
{"user": {"name": "Holden", "location": "San Francisco"}, "text": "Nice day out today"}
{"user": {"name": "Matei", "location": "Berkeley"}, "text": "Even nicer here :)"}
// 从中选取 username（用户名）和 text（文本）字段
```
```python
# 例 5-34：在 Python 中使用 Spark SQL 读取 JSON 数据
tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
results = hiveCtx.sql("SELECT user.name, text FROM tweets")
```
```java
// 例 5-35：在 Scala 中使用 Spark SQL 读取 JSON 数据
val tweets = hiveCtx.jsonFile("tweets.json")
tweets.registerTempTable("tweets")
val results = hiveCtx.sql("SELECT user.name, text FROM tweets")
```
```java
// 例 5-36：在 Java 中使用 Spark SQL 读取 JSON 数据
SchemaRDD tweets = hiveCtx.jsonFile(jsonFile);
tweets.registerTempTable("tweets");
SchemaRDD results = hiveCtx.sql("SELECT user.name, text FROM tweets");
```

##### 数据库
  - Java数据库连接
    - 可以从任何支持Java数据库连接（JDBC）的关系型数据库中读取数据，包括MySQL、Postgre等系统。
    - 需要构建一个 org.apache.spark.rdd.JdbcRDD，将 SparkContext 和其他参数一起传给它
```java
// Scala 中的 JdbcRDD
def createConnection() = {
  Class.forName("com.mysql.jdbc.Driver").newInstance();
  DriverManager.getConnection("jdbc:mysql://localhost/test?user=holden");
}

def extractValues(r: ResultSet) = {
  (r.getInt(1), r.getString(2))
}

val data = new JdbcRDD(sc,
  createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?",
  lowerBound = 1, upperBound = 3, numPartitions = 2, mapRow = extractValues)
println(data.collect().toList)

// JdbcRDD接收这样几个参数: 
// • 首先，要提供一个用于对数据库创建连接的函数。这个函数让每个节点在连接必要的配置后创建自己读取数据的连接。
// • 接下来，要提供一个可以读取一定范围内数据的查询，以及查询参数中 lowerBound 和upperBound 的值。这些参数可以让 Spark 在不同机器上查询不同范围的数据，这样就不会因尝试在一个节点上读取所有数据而遭遇性能瓶颈。
// • 这个函数的最后一个参数是一个可以将输出结果从 java.sql.ResultSet（http://docs.oracle.com/javase/7/docs/api/java/sql/ResultSet.html）转为对操作数据有用的格式的函数。在例 5-37 中，我们会得到 (Int, String) 对。如果这个参数空缺，Spark 会自动将每行结果转为一个对象数组。
```
  - Cassandra
    - 用于 Spark 的 Cassandra 连接器（https://github.com/datastax/spark-cassandra-connector）
    - 你需要添加一些额外的依赖到你的构建文件中才能使用它. Cassandra 还没有使用Spark SQL，不过它会返回由 CassandraRow 对象组成的 RDD，这些对象有一部分方法与Spark SQL 的 Row 对象的方法相同
    - Spark 的 Cassandra 连接器目前只能在 Java 和 Scala 中使用
  - HBase
    - 由 于 org.apache.hadoop.hbase.mapreduce.TableInputFormat 类 的 实 现，Spark 可 以 通 过Hadoop 输入格式访问 HBase。这个输入格式会返回键值对数据，其中键的类型为 org.apache.hadoop.hbase.io.ImmutableBytesWritable，而值的类型为 org.apache.hadoop.hbase.client.Result。Result 类包含多种根据列获取值的方法，在其 API 文档（https://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/Result.html ）中有所描述
```java
// HBase 读取数据的 Scala 示例
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE, "tablename") // 扫描哪张表

val rdd = sc.newAPIHadoopRDD(
  conf, classOf[TableInputFormat], classOf[ImmutableBytesWritable],classOf[Result])

// TableInputFormat 包含多个可以用来优化对 HBase 的读取的设置项
// TableInputFormat 的 API 文档（http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/mapreduce/TableInputFormat.html）
// 在 HBaseConfiguration 中设置它们
```
  - Elasticsearch
    - Elasticsearch 是一个开源的、基于 Lucene 的搜索系统
    - Elasticsearch 连接器会忽略我们提供的路径信息，而依赖于在 SparkContext 中设置的配置项
    - OutputFormat 连接器也没有用到 Spark 所封装的类型，所以我们使用 saveAsHadoopDataSet 来代替
```java
// 在 Scala 中使用 Elasticsearch 输出
val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set("mapred.output.format.class", "org.elasticsearch.hadoop.mr.EsOutputFormat")
jobConf.setOutputCommitter(classOf[FileOutputCommitter])
jobConf.set(ConfigurationOptions.ES_RESOURCE_WRITE, "twitter/tweets")
jobConf.set(ConfigurationOptions.ES_NODES, "localhost")
FileOutputFormat.setOutputPath(jobConf, new Path("-"))
output.saveAsHadoopDataset(jobConf)

// 例 5-47：在 Scala 中使用 Elasticsearch 输入
def mapWritableToInput(in: MapWritable): Map[String, String] = {
  in.map{case (k, v) => (k.toString, v.toString)}.toMap
}

val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set(ConfigurationOptions.ES_RESOURCE_READ, args(1))
jobConf.set(ConfigurationOptions.ES_NODES, args(2))
val currentTweets = sc.hadoopRDD(jobConf,
  classOf[EsInputFormat[Object, MapWritable]], classOf[Object],
  classOf[MapWritable])
// 仅提取map
// 将MapWritable[Text, Text]转为Map[String, String]
val tweets = currentTweets.map{ case (key, value) => mapWritableToInput(value) }

// 就输出而言，Elasticsearch 可以进行映射推断，但是偶尔会推断出不正确的数据类型，因此如果你要存储字符串以外的数据类型，最好明确指定类型映射（https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-put-mapping.html）
```