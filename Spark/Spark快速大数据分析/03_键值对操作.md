

### 键值对操作

#### 动机

- pair RDD: 很多程序的构成要素，它们提供了并行操作各个键或跨节点重新进行数据分组的操作接口
- 通常从一个 RDD 中提取某些字段（例如代表事件时间、用户 ID 或者其他标识符的字段），并使用这些字段作为 pair RDD 操作中的键

#### 创建 pair RDD

- 需要把一个普通的 RDD 转为 pair RDD 时，可以调用 map() 函数来实现，传递的函数需要返回键值对
- 在 Python 中，为了让提取键之后的数据能够在函数中使用，需要返回一个由二元组组成的 RDD
```
# 在 Python 中使用第一个单词作为键创建出一个 pair RDD
pairs = lines.map(lambda x: (x.split(" ")[0], x))
```

- 当用 Scala 和 Python 从一个内存中的数据集创建 pair RDD 时，只需要对这个由二元组组成的集合调用 SparkContext.parallelize() 方法

#### pair RDD的转化操作

![Pair RDD的转化操作](D:\MyDocuments\Typora\spark\Spark快速大数据分析\Pair RDD的转化操作.PNG)

![针对两个Pair RDD的转化操作](D:\MyDocuments\Typora\spark\Spark快速大数据分析\针对两个Pair RDD的转化操作.PNG)

- Pair RDD 也还是 RDD(元素为Python 中的元组), 支持 RDD 所支持的函数

  ```
  # 用 Python 对第二个元素进行筛选
  result = pairs.filter(lambda keyValue: len(keyValue[1]) < 20)
  ```

- 只想访问 pair RDD 的值部分，这时操作二元组很麻烦 -- mapValues(func), 功能类似于 map{case (x, y): (x, func(y))}

  1. 聚合操作

     - reduceByKey()：为数据集中的每个键进行并行的归约操作，每个归约操作会将键相同的值合并起来，返回一个由各键和对应键归约出来的结果值组成的新的 RDD

     - foldByKey()：使用一个与 RDD 和合并函数中的数据类型相同的零值作为初始值，操作所使用的合并函数对零值与另一个元素进行合并，结果仍为该元素

       ```python 3
       # 在 Python 中使用 reduceByKey() 和 mapValues() 计算每个键对应的平均值
       rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
       
       # Python 实现单词计数
       rdd = sc.textFile("s3://...")
  words = rdd.flatMap(lambda x: x.split(" "))
       result = words.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)
       ```
       ![求平均值的数据流](D:\MyDocuments\Typora\spark\Spark快速大数据分析\求平均值的数据流.PNG)
       
       <!-- 调用 reduceByKey() 和 foldByKey() 会在为每个键计算全局的总结果之前先自动在每台机器上进行本地合并。用户不需要指定合并器。更泛化的combineByKey() 接口可以让你自定义合并的行为 -->
       
     - combineByKey() 是最为常用的基于键进行聚合的函数, 类似 aggregate()，可以让用户返回与输入数据的类型不同的返回值
       
       - combineByKey() 会遍历分区中的所有元素，因此每个元素的键要么还没有遇到过，要么就和之前的某个元素的键相同
       
       - 如果这是一个新的元素，combineByKey() 会使用一个叫作 createCombiner() 的函数来创建那个键对应的累加器的初始值，这一过程会在每个分区中第一次出现各个键时发生
       
       - 如果这是一个在处理当前分区之前已经遇到的键，它会使用 mergeValue() 方法将该键的累加器对应的当前值与这个新的值进行合并
       
       - 由于每个分区都是独立处理的，因此对于同一个键可以有多个累加器。如果有两个或者更多的分区都有对应同一个键的累加器，就需要使用用户提供的 mergeCombiners() 方法将各个分区的结果进行合并
       
       - 如果已知数据在进行 combineByKey() 时无法从 map 端聚合中获益的话，可以禁用它。如果希望禁用 map 端组合，就需要指定分区方式。就目前而言，你可以通过传递 rdd.partitioner 来直接使用源 RDD 的分区方式
       
         ```
         # 在 Python 中使用 combineByKey() 求每个键对应的平均值
         sumCount = nums.combineByKey((lambda x: (x,1)),
                                       (lambda x, y: (x[0] + y, x[1] + 1)),
                                       (lambda x, y: (x[0] + y[0], x[1] + y[1])))
         sumCount.map(lambda key, xy: (key, xy[0]/xy[1])).collectAsMap()
         ```
       
         ![combineByKey() 数据流示意图](D:\MyDocuments\Typora\spark\Spark快速大数据分析\combineByKey() 数据流示意图.PNG)
       
     - 并行度调优
     
       - 每个 RDD 都有固定数目的分区，分区数决定了在 RDD 上执行操作时的并行度
     
       - 在执行聚合或分组操作时，可以要求 Spark 使用给定的分区数
     
       - Spark 始终尝试根据集群的大小推断出一个有意义的默认值，但是有时候你可能要对并行度进行调优来获取更好的性能表现
     
       - 本章讨论的大多数操作符都能接收第二个参数，这个参数用来指定分组结果或聚合结果的RDD 的分区数
     
         ```
         # 在 Python 中自定义 reduceByKey() 的并行度
         data = [("a", 3), ("b", 4), ("a", 1)]
         sc.parallelize(data).reduceByKey(lambda x, y: x + y) # 默认并行度
         sc.parallelize(data).reduceByKey(lambda x, y: x + y, 10) # 自定义并行度
         ```
     
       - repartition()：在除分组操作和聚合操作之外的操作中也能改变 RDD 的分区，它会把数据通过网络进行混洗，并创建出新的分区集合
     
       - coalesce()：优化版的repartition()，你可以使用 Python 中的 rdd.getNumPartitions 查看 RDD 的分区数，并确保调用 coalesce() 时将 RDD 合并到比现在的分区数更少的分区中
     
  2. 数据分组

     - 将数据根据键进行分组：

       - 如果数据已经以预期的方式提取了键，groupByKey() 就会使用 RDD 中的键来对数据进行分组；对于一个由类型 K 的键和类型 V 的值组成的 RDD，所得到的结果 RDD 类型会是[K, Iterable[V]]

       - groupBy() 可以用于未成对的数据上，也可以根据除键相同以外的条件进行分组。它可以
         接收一个函数，对源 RDD 中的每个元素使用该函数，将返回结果作为键再进行分组

       - 先使用 groupByKey() 然后再对值使用 reduce() 或者fold() —— 很有可能可以通过使用一种根据键进行聚合的函数来更高效地实现同样的效果。对每个键归约数据，返回对应每个键的归约值的RDD，而不是把 RDD 归约为一个内存中的值

         ```python 3
         # rdd.reduceByKey(func) 与 rdd.groupByKey().mapValues(value => value.reduce(func)) 等价
         # 但是前者更为高效，因为它避免了为每个键创建存放值的列表的步骤
         ```

       - cogroup() ：对多个共享同一个键的 RDD 进行分组

         - 对两个键的类型均为 K 而值的类型分别为 V 和 W 的 RDD 进行cogroup() 时，得到的结果 RDD 类型为 [(K, (Iterable[V], Iterable[W]))]。
         - 如果其中的一个 RDD 对于另一个 RDD 中存在的某个键没有对应的记录，那么对应的迭代器则为空。
         - cogroup() 提供了为多个 RDD 进行数据分组的方法。
         - 连接操作的构成要素，不仅可以用于实现连接操作，还可以用来求键的交集。除此之外，cogroup() 还能同时应用于三个及以上的 RDD

  3. 连接

     - join 操作符表示内连接
     - leftOuterJoin(other)和 rightOuterJoin(other), 在 Python 中，如果一个值不存在，则使用 None 来表示
       - 调用 isPresent() 来看值是否存在，如果数据存在，则可以调用 get() 来获得其中包含的对象实例
     
  4. 数据排序

     -  sortByKey()， ascending: 是否想要让结果按升序排序

       ```SPARQL
       # 在 Python 中以字符串顺序对整数进行自定义排序
       rdd.sortByKey(ascending=True, numPartitions=None, keyfunc = lambda x: str(x))
       ```

#### Pair RDD的行动操作
| 函数 | 描述 | 示例 | 结果 |
| ---- | ---- | ---- | ---- |
| countByKey() | 对每个键对应的元素分别计数 | rdd.countByKey() | {(1, 1), (3, 2)} |
| collectAsMap() | 将结果以映射表的形式返回，以便查询 | rdd.collectAsMap() | Map{(1, 2), (3, 4), (3, 6)} |
| lookup(key) | 返回给定键对应的所有值 | rdd.lookup(3) | [4, 6] |

#### 数据分区（进阶）
##### partitionBy
- 对 userData 表使用 partitionBy() 转化操作，将这张表转为哈希分区通过向 partitionBy 传递一个 spark.HashPartitioner 对象
	- 当调用 userData.join(events) 时，Spark 只会对 events 进行数据混洗操作
	- 将 events 中特定 UserID 的记录发送到 userData 的对应分区所在的那台机器上
	- 需要通过网络传输的数据就大大减少了，程序运行速度也可以显著提升了
- partitionBy() 是一个转化操作，因此它的返回值总是一个新的 RDD，但它不会改变原来的 RDD
- RDD 一旦创建就无法修改。因此应该对 partitionBy() 的结果进行持久化，并保存为 userData
	- 不进行持久化会导致整个 RDD 谱系图重新求值，partitionBy() 带来的好处就会被抵消，导致重复对数据进行分区以及跨节点的混洗，和没有指定分区方式时发生的情况十分相似
- 传给 partitionBy() 的100 表示分区数目，它会控制之后对这个 RDD 进行进一步操作（比如连接操作）时有多少任务会并行执行，这个值至少应该和集群中的总核心数一样
- 许多其他 Spark 操作会自动为结果 RDD 设定已知的分区方式信息，而且除join() 外还有很多操作也会利用到已有的分区信息
  - sortByKey() 和 groupByKey()会分别生成范围分区的 RDD 和哈希分区的 RDD
- map() 这样的操作会导致新的 RDD 失去父 RDD 的分区信息，因为这样的操作理论上可能会修改每条记录的键

##### 　获取RDD的分区方式
- 在 Scala 和 Java 中，你可以使用 RDD 的 partitioner 属性来获取 RDD 的分区方式

##### 从分区中获益的操作

cogroup()、groupWith()、join()、leftOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey() 以及 lookup()

##### 影响分区方式的操作

会为生成的结果 RDD 设好分区方式的操作：cogroup()、groupWith()、join()、lef tOuterJoin()、rightOuterJoin()、groupByKey()、reduceByKey()、combineByKey()、partitionBy()、sort()、mapValues()（如果父 RDD 有分区方式的话）、flatMapValues()（如果父 RDD 有分区方式的话），以及 filter()（如果父 RDD 有分区方式的话）

对于二元操作，输出数据的分区方式取决于父 RDD 的分区方式。默认情况下，结
果会采用哈希分区，分区的数量和操作的并行度一样。如果其中的一个父 RDD 已
经设置过分区方式，那么结果就会采用那种分区方式；如果两个父 RDD 都设置过分区方
式，结果 RDD 会采用第一个父 RDD 的分区方式

PageRank 算法

##### 自定义分区方式

Spark 提供的 HashPartitioner 与 RangePartitioner 已经能够满足大多数用例，但Spark 还是允许你通过提供一个自定义的 Partitioner 对象来控制 RDD 的分区方式

要实现自定义的分区器，你需要继承 org.apache.spark.Partitioner 类并实现下面三个方法。
• numPartitions: Int：返回创建出来的分区数。
• getPartition(key: Any): Int：返回给定键的分区编号（0 到 numPartitions-1）。
• equals()：Java 判断相等性的标准方法。这个方法的实现非常重要，Spark 需要用这个方法来检查你的分区器对象是否和其他分区器实例相同，这样 Spark 才可以判断两个RDD 的分区方式是否相同

在 Python 中，不需要扩展 Partitioner 类，而是把一个特定的哈希函数作为一个额外的参
数传给 RDD.partitionBy() 函数

```python 3
# Python 自定义分区方式
import urlparse
def hash_domain(url):
     return hash(urlparse.urlparse(url).netloc)
rdd.partitionBy(20, hash_domain) # 创建20个分区
```

